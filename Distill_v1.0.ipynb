{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c55a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyppeteer import launch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "\n",
    "import asyncio\n",
    "import datetime\n",
    "import difflib\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99bb1bb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Websites:\n",
      "http://shielded-harbor-71309.herokuapp.com/login-pc\n",
      "\n",
      "\n",
      "Any new websites to input? (y/n)y\n",
      "How many?2\n",
      "Full link of new website: https://main.scta.org.sg/\n",
      "Full link of new website: http://scanme.nmap.org/\n"
     ]
    }
   ],
   "source": [
    "#Confirm list of websites to currently monitor\n",
    "list_of_websites = ['http://shielded-harbor-71309.herokuapp.com/login-pc']\n",
    "print(\"Current Websites:\")\n",
    "for website in list_of_websites:\n",
    "    print(website)\n",
    "\n",
    "print(\"\\n\")\n",
    "action = input(\"Any new websites to input? (y/n)\").strip()\n",
    "if action == 'y':\n",
    "    number = int(input(\"How many? \").strip())\n",
    "    for i in range(number):\n",
    "        new_website = input(\"Full link of new website: \").strip()\n",
    "        list_of_websites.append(new_website)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if report is wanted, and if yes, since what date and time.\n",
    "report_wanted = False\n",
    "\n",
    "action = input(\"Do you want to generate a report? (y/n)\").strip()\n",
    "if action == 'y':\n",
    "    report_wanted = True\n",
    "    global start_datetime = input(\"Start date and time (24h format): (yyyy-mm-dd hh_mm)\").strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping of website for screenshot, html and textcontent\n",
    "async def main(website):\n",
    "    browser = await launch(headless=True)\n",
    "    page = await browser.newPage()\n",
    "\n",
    "    await page.goto(website, waitUntil=\"load\")\n",
    "    \n",
    "    \n",
    "    #Creates folders if this is a new webpage\n",
    "    newpath = website.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    newpath = newpath.replace(\"/\", \"_\").strip(\"_\")\n",
    "    if not os.path.exists(\"screenshots/\" + newpath):\n",
    "        os.makedirs(\"screenshots/\" + newpath)\n",
    "        os.makedirs(\"screenshots/\" + newpath + \"/html\")\n",
    "        os.makedirs(\"screenshots/\" + newpath + \"/content\")\n",
    "        os.makedirs(\"screenshots/\" + newpath + \"/images\")\n",
    "        \n",
    "    if not os.path.exists(\"defacement/\" + newpath):\n",
    "        os.makedirs(\"defacement/\" + newpath)\n",
    "        os.makedirs(\"defacement/\" + newpath + \"/html\")\n",
    "        os.makedirs(\"defacement/\" + newpath + \"/content\")\n",
    "        os.makedirs(\"defacement/\" + newpath + \"/images\")\n",
    "\n",
    "\n",
    "    #Scraping of website\n",
    "    filename = f\"screenshots/{newpath}/images/{datetime.datetime.now()}.png\".replace(\":\", \"_\")\n",
    "    await page.screenshot({'path': filename, 'fullPage': True}) #Screenshot page\n",
    "    textcontent = await page.evaluate('document.body.textContent', force_expr=True) #Scraping textcontent\n",
    "    html = await page.content() #Scraping html \n",
    "    \n",
    "    #Writing scraped content into files\n",
    "    with open(f\"screenshots\\\\{newpath}\\\\content\\\\{datetime.datetime.now()}.txt\".replace(\":\", \"_\"), \"w+\") as file:\n",
    "        file.write(textcontent)\n",
    "    \n",
    "    with open(f\"screenshots\\\\{newpath}\\\\html\\\\{datetime.datetime.now()}.html\".replace(\":\", \"_\"), \"w+\") as file:\n",
    "        file.write(html)\n",
    "        \n",
    "    await browser.close()\n",
    "\n",
    "    \n",
    "#for website in list_of_websites:\n",
    "for website in list_of_websites:\n",
    "    asyncio.get_event_loop().run_until_complete(main(website))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing screenshots for each website\n",
    "\n",
    "for website in list_of_websites:\n",
    "    # Load the OpenAI CLIP Model\n",
    "    print('Loading CLIP Model...')\n",
    "    model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "    # Next we compute the embeddings\n",
    "    # To encode an image, you can use the following code:\n",
    "    # from PIL import Image\n",
    "    # encoded_image = model.encode(Image.open(filepath))\n",
    "    image_names = list(glob.glob(f'screenshots/{website}/images/*.png'))[-2:] #compare 2 most recent files\n",
    "    print(\"Images:\", len(image_names))\n",
    "    encoded_image = model.encode([Image.open(filepath) for filepath in image_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    # Now we run the clustering algorithm. This function compares images aganist \n",
    "    # all other images and returns a list with the pairs that have the highest \n",
    "    # cosine similarity score\n",
    "    processed_images = util.paraphrase_mining_embeddings(encoded_image)\n",
    "    NUM_SIMILAR_IMAGES = 10 \n",
    "\n",
    "    # =================\n",
    "    # DUPLICATES\n",
    "    # =================\n",
    "    print('Finding duplicate images...')\n",
    "    # Filter list for duplicates. Results are triplets (score, image_id1, image_id2) and is scorted in decreasing order\n",
    "    # A duplicate image will have a score of 1.00\n",
    "    # It may be 0.9999 due to lossy image compression (.jpg)\n",
    "    duplicates = [image for image in processed_images if image[0] >= 0.999]\n",
    "\n",
    "    # Output the top X duplicate images\n",
    "    for score, image_id1, image_id2 in duplicates[0:NUM_SIMILAR_IMAGES]:\n",
    "        print(\"\\nScore: {:.3f}%\".format(score * 100))\n",
    "        print(image_names[image_id1])\n",
    "        print(image_names[image_id2])\n",
    "\n",
    "    # =================\n",
    "    # NEAR DUPLICATES\n",
    "    # =================\n",
    "    print('Finding near duplicate images...')\n",
    "    # Use a threshold parameter to identify two images as similar. By setting the threshold lower, \n",
    "    # you will get larger clusters which have less similar images in it. Threshold 0 - 1.00\n",
    "    # A threshold of 1.00 means the two images are exactly the same. Since we are finding near \n",
    "    # duplicate images, we can set it at 0.99 or any number 0 < X < 1.00.\n",
    "    threshold = 0.99\n",
    "    near_duplicates = [image for image in processed_images if image[0] < threshold]\n",
    "\n",
    "    #for score, image_id1, image_id2 in near_duplicates[0:NUM_SIMILAR_IMAGES]:\n",
    "        #print(\"\\nScore: {:.3f}%\".format(score * 100))\n",
    "        #print(image_names[image_id1])\n",
    "        #print(image_names[image_id2])\n",
    "\n",
    "        if score < 1.0:\n",
    "            print(\"Changes detected in screenshot.\\n\")\n",
    "            with open(f\"defacement\\\\{website}\\\\images\\\\Changes detected at {datetime.datetime.now()}.txt\".replace(\":\", \"_\"), \"w+\") as file:\n",
    "                file.write(\"\\nScore: {:.3f}%\\n\".format(score * 100))\n",
    "                file.write(image_names[image_id1] + \"\\n\")\n",
    "                file.write(image_names[image_id2] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing html files\n",
    "\n",
    "for website in list_of_websites:\n",
    "    #Compare 2 most recent html files\n",
    "    html_names = list(glob.glob(f'screenshots/{website}/html/*.html'))[-2:]\n",
    "    print(\"Files:\", len(html_names))\n",
    "\n",
    "    with open(html_names[0]) as file_1:\n",
    "        file_1_text = file_1.readlines()\n",
    "\n",
    "    with open(html_names[1]) as file_2:\n",
    "        file_2_text = file_2.readlines()\n",
    "\n",
    "    # Find and save the diff:\n",
    "    diff = \"\"\n",
    "    for line in difflib.unified_diff(file_1_text, file_2_text, fromfile='file1.txt', tofile='file2.txt', lineterm=''):\n",
    "        diff += line + \"\\n\"\n",
    "\n",
    "    if diff:\n",
    "        print(\"Changes detected in html.\\n\")\n",
    "        with open(f\"defacement\\\\{website}\\\\html\\\\Changes detected at {datetime.datetime.now()}.txt\".replace(\":\", \"_\"), \"w+\") as file:\n",
    "            file.write(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing textcontent files\n",
    "\n",
    "for website in list_of_websites:\n",
    "    #Compare 2 most recent content files    \n",
    "    content_names = list(glob.glob(f'screenshots/{website}/content/*.txt'))[-2:]\n",
    "    print(\"Files:\", len(content_names))\n",
    "\n",
    "    with open(content_names[0]) as file_1:\n",
    "        file_1_text = file_1.readlines()\n",
    "\n",
    "    with open(content_names[1]) as file_2:\n",
    "        file_2_text = file_2.readlines()\n",
    "\n",
    "    # Find and save the diff:\n",
    "    diff = \"\"\n",
    "    for line in difflib.unified_diff(file_1_text, file_2_text, fromfile='file1.txt', tofile='file2.txt', lineterm=''):\n",
    "        diff += line +\"\\n\"\n",
    "\n",
    "    if diff:\n",
    "        print(\"Changes detected in textcontent.\\n\")\n",
    "        with open(f\"defacement\\\\{website}\\\\content\\\\Changes detected at {datetime.datetime.now()}.txt\".replace(\":\", \"_\"), \"w+\") as file:\n",
    "            file.write(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db956330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbfe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#References\n",
    "\n",
    "\"\"\"\n",
    "1. https://www.geeksforgeeks.org/compare-two-files-line-by-line-in-python/\n",
    "2. https://github.com/pyppeteer/pyppeteer\n",
    "3. https://stackoverflow.com/questions/11541154/checking-images-for-similarity-with-opencv/71634759#71634759\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
